{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "## Basics of PyTorch\n",
    "- Defining network architecture\n",
    "- Dataset and Dataloader\n",
    "- Training\n",
    "- Inference\n",
    "\n",
    "## Data Augmentation\n",
    "- Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Massachusetts Buildings Dataset\n",
    "In this tutorial, we use the dataset for building detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fpath_image = './dataset/building_vmnih/test/image/22828930_15.tiff'\n",
    "fpath_label = './dataset/building_vmnih/test/label/22828930_15.tif'\n",
    "\n",
    "image = np.array(Image.open(fpath_image))\n",
    "label = np.array(Image.open(fpath_label))\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(image, interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.show()\n",
    "ax.set_title('Source image')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(label, interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Ground truth')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, 137 pairs of images and labels like above are provided for training. Before training models on the dataset, we explain some basics of pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basics of PyTorch\n",
    "PyTorch is a famous deep learning framework developed by Facebook. Here, we introduce the basics of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, import pytorch modules. <br>\n",
    "`torch.nn` contains classes for various kinds of operations such as convolution, pooling, batchnormalization, etc.<br>\n",
    "`torch.nn.functional` contains functions for basic (and non-parametric) operations such as activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors in PyTorch\n",
    "We can make tensors in many ways. Tensors can be used like numpy array.\n",
    "```\n",
    "torch.FloatTensor([1,2,3])\n",
    "torch.from_numpy(ndarray)\n",
    "torch.Tensor([1,2,3]).float()\n",
    "```\n",
    "\n",
    "### Basic operations\n",
    "Before implementing network, we briefly check how to define basic operations like convolution or pooling. <br><br>\n",
    "\n",
    "**Convolutional layer**<br>\n",
    "convolutional layer is implemented as a class (`nn.Conv2d`). First create an instance of the layer, then use it like a function.\n",
    "```\n",
    "conv1 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "output = conv1(input)\n",
    "```\n",
    "\n",
    "**Pooling layer**\n",
    "```\n",
    "pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "output = pool1(input)\n",
    "\n",
    "pool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "output = pool2(input)\n",
    "```\n",
    "\n",
    "**Fully connected layer**\n",
    "```\n",
    "fc1 = torch.nn.Linear(in_features=128, out_features=128)\n",
    "output = fc1(input)\n",
    "```\n",
    "\n",
    "**Activation functions**<br>\n",
    "Activations are implemented as functions.\n",
    "```\n",
    "output = torch.nn.functional.relu(input)\n",
    "```\n",
    "We can also use class version.\n",
    "```\n",
    "act1 = torch.nn.ReLU()\n",
    "output = act1(input)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining network architecture\n",
    "Using the basic operations mentioned above, we implement the VGG-like model which has $64\\times64$ patches as inputs and output the estimation of center area of size $16\\times16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGGS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGS, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool3   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.pool4   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc6 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc7 = nn.Linear(in_features=1024, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=256)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.dropout(self.fc6(x))\n",
    "        x = F.dropout(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        x = x.view(batch_size, 16, 16)\n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model, and check the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGGS()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the architecture is correct by processing random tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.randn(100,3,64,64).astype(np.float32)\n",
    "inputs = torch.from_numpy(inputs)\n",
    "output = model(inputs)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "Here, we define cross entropy loss function for training. We need a small value `eps` to avoid overflow at `log()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Criterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Criterion, self).__init__()\n",
    "        self.eps = 1.0e-7\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        prob = F.sigmoid(inputs)\n",
    "        loss = -targets.float()*(prob + self.eps).log() - (1-targets.float())*(1 - prob + self.eps).log()\n",
    "        loss = loss.mean()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader\n",
    "Bellow figure presents how mini-batches for training are created in PyTorch. <br>\n",
    "**DataLoader**: DataLoader picks up data from Dataset and pack the collected data into mini-batch. <br>\n",
    "**Dataset**: Dataset load and pre-process data requested from DataLoader. In some cases, we need to implement the Dataset custamized for our own data. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/pth_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our own Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Buildings(Dataset):\n",
    "    def __init__(self, fpath_image_npy, fpath_label_npy):\n",
    "        self.images = np.load(fpath_image_npy)\n",
    "        self.labels = np.load(fpath_label_npy)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        image = image.transpose([2,0,1])    # (H,W,B) to (B,H,W)\n",
    "        \n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        label_tensor = torch.from_numpy(label).long()\n",
    "        \n",
    "        return image_tensor, label_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Buildings('./dataset/building_vmnih/train/patches/sat.npy', './dataset/building_vmnih/train/patches/map.npy')\n",
    "image_tensor, label_tensor = dataset[0]\n",
    "print(image_tensor.size())\n",
    "print(label_tensor.size())\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set DataLoader and check if it works. <br><br>\n",
    "Arguments of init for DataLoader: <br>\n",
    "batch_size: specify batch_size<br>\n",
    "shuffle: if True, the data is randomly sampled from the Dataset and if False, the data is sequentially sampled.<br>\n",
    "num_workers: number fo multi-thread workers. We can load data in parallel which will speed up dataloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "for image, label in loader:\n",
    "    print(image.size(), torch.mean(image).item())\n",
    "    print(label.size(), label[0,0,0].item())\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we prepared the training dataloader. In the following we briefly demonstrate training procedure. Since the full training takes much time, we just demonstrate first several iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device. In this case, \"cpu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set maximum epoch to train. For demonstration, we set the max_epochs 1, however we recommend to use tens of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer model to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set optimizer. Here, we use Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.999), weight_decay=1.0e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set loss function, and transfer it to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Criterion().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training iterations. We demonstrate only several iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    for idx, (image, label) in enumerate(loader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        \n",
    "        # At the begining of each iteraton, clear accumulated gradient for the network parameters.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Input the image to the model and get the prediction.\n",
    "        output = model(image)\n",
    "        \n",
    "        # Calculate the loss function comparing the prediction and the ground truth\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # Backpropagate the error signal through the model to calculate gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters using the calculated gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch #%d, batch #%d: loss=%f' % (epoch, idx, loss.item()))\n",
    "        \n",
    "        # For demonstration, stop iteration at iter5\n",
    "        if idx == 4:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = './learned_weights/'\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the learned weights as dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_weights = model.state_dict()\n",
    "for key, weight in dict_weights.items():\n",
    "    print('%s\\t%s' % (key, weight.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dict_weights, output_dir + '/demo_building_vggs.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings. We will crop the test image by sliding the cropping window by stride of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 64\n",
    "aoi_size = 16\n",
    "stride = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/fig_test_patch_small.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare test area image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Load image\n",
    "fpath_test_image = './dataset/building_vmnih/test/image/22828930_15.tiff'\n",
    "image = np.array(Image.open(fpath_test_image))\n",
    "org_height, org_width, _ = image.shape\n",
    "\n",
    "# Normalize\n",
    "mean = np.mean(image, axis=(0,1), keepdims=True)\n",
    "std = np.std(image, axis=(0,1), keepdims=True)\n",
    "image = (image - mean) / std\n",
    "\n",
    "# To tensor\n",
    "image = image.transpose([2,0,1])    # Swap dimensions, (H,W,B) to (B,H,W)\n",
    "image = image[np.newaxis,:,:,:]    # Add batch dimension (N,B,H,W)\n",
    "image = torch.from_numpy(image).float()    # Convert to float tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the network outputs the estimation results of the center area of the input patches, to estimate the edge area of the image, a part of the input patch lies out of valid image region. Therefore we need expand the original image using some padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/fig_test_padding_small.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reflection padding\n",
    "pad_size = int((64-16)/2)\n",
    "pad = nn.ReflectionPad2d(pad_size)\n",
    "image = pad(image)\n",
    "_, _, height, width = image.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model and load the trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model\n",
    "model = VGGS().to(device)\n",
    "\n",
    "# Load learned weights\n",
    "learned_weights = torch.load('./learned_weights/demo_building_vggs.torch')\n",
    "model.load_state_dict(learned_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set place holder\n",
    "prob_map = np.zeros([height, width])\n",
    "count_map = np.zeros([height, width])\n",
    "\n",
    "# Crop patches in sliding manner, and apply the prediction model\n",
    "num_tiles_x = (width - patch_size) // stride + 2\n",
    "num_tiles_y = (height - patch_size) // stride + 2\n",
    "\n",
    "for iy in range(num_tiles_y):\n",
    "    for ix in range(num_tiles_x):\n",
    "        print('(%d/%d, %d/%d)' % (iy, num_tiles_y, ix, num_tiles_x))\n",
    "        ulx = ix * stride\n",
    "        uly = iy * stride\n",
    "        lrx = ulx + patch_size\n",
    "        lry = uly + patch_size\n",
    "        \n",
    "        if lrx > width:\n",
    "            ulx = width - patch_size\n",
    "            lrx = width\n",
    "            \n",
    "        if lry > height:\n",
    "            uly = height - patch_size\n",
    "            lry = height\n",
    "            \n",
    "        patch = image[:, :, uly:lry, ulx:lrx]\n",
    "        patch = patch.to(device)\n",
    "        \n",
    "        logit = model(patch)\n",
    "        prob = F.sigmoid(logit)\n",
    "        \n",
    "        stx = ulx + int((patch_size - aoi_size) / 2)\n",
    "        sty = uly + int((patch_size - aoi_size) / 2)\n",
    "        prob_map[sty:sty+aoi_size, stx:stx+aoi_size] += prob.detach().cpu().numpy().squeeze()\n",
    "        count_map[sty:sty+aoi_size, stx:stx+aoi_size] += np.ones([aoi_size,aoi_size])\n",
    "\n",
    "# Eliminate padded region\n",
    "prob_map = prob_map[pad_size:pad_size+org_height, pad_size:pad_size+org_width]\n",
    "count_map = count_map[pad_size:pad_size+org_height, pad_size:pad_size+org_width]\n",
    "\n",
    "# Take average for overlaped regions\n",
    "result = prob_map / count_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the estimation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fpath_test_label = './dataset/building_vmnih/test/label/22828930_15.tif'\n",
    "label = np.array(Image.open(fpath_test_label))[:,:,0]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(result, interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.show()\n",
    "ax.set_title('Estimation')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(label, interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Ground truth')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred = (result > 0.5)\n",
    "label = (label > 0.5)\n",
    "\n",
    "TP = np.sum(pred*label)\n",
    "T = np.sum(label)\n",
    "P = np.sum(pred)\n",
    "IoU = float(TP) / (T+P-TP) * 100\n",
    "print('IoU: %.2f%%' % IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add data augmentation (random rotation and flipping) to our Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "class Buildings(Dataset):\n",
    "    def __init__(self, fpath_image_npy, fpath_label_npy, augmentation=False):\n",
    "        self.images = np.load(fpath_image_npy)\n",
    "        self.labels = np.load(fpath_label_npy)\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Augmentation\n",
    "        if self.augmentation:\n",
    "            # Randomly choose rotation angle and flipping direction\n",
    "            rotation = random.choice([0, 90, 180, 270])\n",
    "            flip = random.choice(['H', 'V', 'N'])\n",
    "            \n",
    "            # Apply transformation for image\n",
    "            image = self._rotate(image, rotation)\n",
    "            image = np.array(self._flip(image, flip))\n",
    "\n",
    "            # Also, apply the same transformation for label\n",
    "            label = self._rotate(label, rotation)\n",
    "            label = np.array(self._flip(label, flip))\n",
    "        \n",
    "        image = image.transpose([2,0,1])    # (H,W,B) to (B,H,W)\n",
    "        \n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        label_tensor = torch.from_numpy(label).long()\n",
    "        \n",
    "        return image_tensor, label_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    # Rotation function\n",
    "    def _rotate(self, image, rotation):\n",
    "        if rotation == 0:\n",
    "            return image\n",
    "        elif rotation == 90:\n",
    "            image = image.swapaxes(0,1)\n",
    "            return np.flip(image, axis=0)\n",
    "        elif rotation == 180:\n",
    "            image = np.flip(image, axis=0)\n",
    "            return np.flip(image, axis=1)\n",
    "        elif rotation == 270:\n",
    "            image = image.swapaxes(0,1)\n",
    "            return np.flip(image, axis=1)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "    # Flip function\n",
    "    def _flip(self, image, direction):\n",
    "        if direction == 'H':\n",
    "            return np.flip(image, axis=1)\n",
    "        elif direction == 'V':\n",
    "            return np.flip(image, axis=0)\n",
    "        elif direction == 'N':\n",
    "            return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the augmentation by sampling some patches from the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib.gridspec import GridSpec\n",
    "grid = GridSpec(nrows=2, ncols=10)\n",
    "\n",
    "\n",
    "mean = np.array([75, 75, 75])\n",
    "std = np.array([50, 50, 50])\n",
    "\n",
    "mean = mean[:, np.newaxis, np.newaxis]\n",
    "std = std[:,np.newaxis, np.newaxis]\n",
    "\n",
    "dataset = Buildings('./dataset/building_vmnih/train/patches/sat.npy', './dataset/building_vmnih/train/patches/map.npy', augmentation=True)\n",
    "\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "for i in range(10):\n",
    "    image_tensor, label_tensor = dataset[3]\n",
    "    image_patch = image_tensor.detach().numpy() * std + mean\n",
    "    label_patch = label_tensor.detach().numpy()\n",
    "    \n",
    "    image_patch = image_patch.transpose([1,2,0])\n",
    "    image_patch = image_patch.clip(0,255).astype(np.uint8)\n",
    "\n",
    "    ax = fig.add_subplot(grid[0,i])\n",
    "    ax.imshow(image_patch, interpolation='none')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    ax = fig.add_subplot(grid[1,i])\n",
    "    ax.imshow(label_patch, interpolation='none')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displaying the result from trained model using large-scale data with more epochs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"./fig/bulding_detct.png\" width =600 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such techniques are effective especially when we have limited amount of labeled data.\n",
    "There are other methods which is \n",
    "- Transfer learning\n",
    "- Data fusion\n",
    "- Dilated convolutions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
