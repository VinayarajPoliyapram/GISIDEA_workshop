{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=black> Theory and Practice of Deep Learning for Large-scale Geospatial Data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Lectures: PoliyapramVinayaraj (email: vinay223333@gmail.com) and Ryuhei Hamaguchi (email: ryuhei.hamaguchi@gmail.com )</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a wide range of growing applicability of deep learninng techniques such speech recognition, image recognition and natural language processing (NLP) and many more such as robot navigation systems, self-driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A brief introduction to Artificial Intelligence Research Center (AIRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/AIRC1.png\" width=600 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Utilizing AI in “global monitoring of the Earth” and “smart mobility”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The innovative downsizing of Earth-observing satellites has realized “constellations” consisting of hundreds of micro-satellites. The resulting huge amount of Earth imagery must be automatically analyzed by AI instead of human eyes. In order to promote the commercialization, we are developing an intelligent system which efficiently collect beneficial information from the tremendous satellite imagery by automatic detection of any kinds of objects and events on the Earth [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/AIRC2.png\" width=400 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=black>  Deep  learning for Remote Sensing </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-driven science and machine-learning techniques are becoming increasingly important. In particular, deep learning has proven to be a major breakthrough and an extremely powerful tool in many fields. This workshop aims mainly to introduce deep learning techniques for Geospatial data application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remote-sensing data are often multimodal, e.g., from optical (multi- and hyperspectral), Lidar, and synthetic aperture radar (SAR) sensors\n",
    "- Remote-sensing data are geolocated, i.e., they are naturally  located in the geographical space\n",
    "- Continous and temporal observations triggering to time-series processing\n",
    "- Remote sensing also faces the “big data” challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=black> Common applications </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Hyperspectral image analysis, \n",
    "   Classification of SAR images, \n",
    "  Multimodal data fusion, Object detection,\n",
    "  3-D reconstruction,\n",
    "  Noise reduction,\n",
    "  Domain adaptation, Dimensionality reduction, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Perceptron Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The perceptron is an intuitive, easy to implement machine learning algorithms which given an input vector x (x1, x2,.., xn) often called input features, outputs either 1 or 0. The perceptron belongs to the category of supervised learning algorithms, more specifically single-layer binary classifier. The perceptron algorithm is about learning the weights for the input features in order to draw linear decision boundary that allows us to discriminate between the two linearly separable classes. In this paper, rather than using it as a classifier, SLP to compute best parameters (weights) which used to multiply the input features in order to provide best accuracy with respect to the ground truth data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Single Layer Perceptron**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/SLP.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$ Net =  w_1 x_1  +w_2 x_2  + w_3 x_3  + ... + w_m x_m  + b $ </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where, 'Net' is the net product,'w' is the weight 'x' is the feature (in case mutispectral images 'x' can be spectral bands) and 'b' is the bias[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Net =  \\sum_{i=0}^m w_i x_i + b \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us check this with using some examples in Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'consider we have five input variables (spectral bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=5, kernel_initializer='random_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron can be  initilized with random weights, several options to initlize such as\n",
    "'random_uniform', 'random_normal', zero, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lets  check the initilized parameters (weights and bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_bias = model.layers[0].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (weights_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center>  Multi Layer Perceptron </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/MLP.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sizing neural networks:** The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with the two example networks in the above picture[3]\n",
    "\n",
    "The first network (left) has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters.\n",
    "The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    " Net_j = \\sum_{i=0}^m w_ji x_i + b \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Where, $ Net_j $ net product for each neuron in the hidden layer <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us check this with using some examples in Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, kernel_initializer='random_uniform'))\n",
    "model.add(Dense(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_bias0 = model.layers[0].get_weights()\n",
    "weights_bias1 = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (weights_bias0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.count_nonzero(weights_bias0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (weights_bias1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (weights_bias1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.count_nonzero(weights_bias1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. National Institute of Advanced Industrial Science and Technology\n",
    "2. F.ROSENBLATT, THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN\n",
    "3. http://cs231n.github.io"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
